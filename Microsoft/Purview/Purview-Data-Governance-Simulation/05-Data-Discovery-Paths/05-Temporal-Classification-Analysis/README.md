# Lab 05-Temporal: Longitudinal Classification Analysis (Optional/Advanced)

## üéØ Lab Objective

Track Microsoft Purview Sensitive Information Type (SIT) classification evolution over time to validate classification stability, measure index maturation patterns, and understand temporal dynamics in enterprise data governance implementations.

> **‚è±Ô∏è Time Commitment**: This is a **2-3 week longitudinal study** requiring periodic scans at 24 hours, 7 days, 14 days, and 21 days after Lab 03 content upload. Each scan requires 15-30 minutes of active time, but **cannot be accelerated** due to Microsoft 365 indexing schedules.

> **üìå Optional Lab Status**: This lab is **supplemental/optional** and is NOT required to proceed to Lab 06 or complete the core Lab 05 discovery method comparison. It provides advanced insights into classification drift and index maturation for users interested in production deployment planning and long-term governance strategy.

---

## üìã Prerequisites

### Required Labs Completed

- **Lab 02**: Document generation with sensitive information types embedded
- **Lab 03**: Multi-site content upload to SharePoint simulation sites
- **Lab 05b**: eDiscovery Compliance Search (24-hour baseline scan)

### Time-Based Prerequisites

**Critical Timing Requirement**: Lab 03 content upload must be completed at least **21 days ago** to run the full temporal analysis. If Lab 03 was completed less than 21 days ago, you can still begin the temporal study but will need to wait for the appropriate intervals.

**Scan Timeline From Lab 03 Upload**:
- **24 hours**: eDiscovery search (Lab 05b baseline) ‚úÖ
- **7 days**: First Graph API scan (Week 1 checkpoint) 
- **14 days**: Second Graph API scan (Week 2 checkpoint)
- **21 days**: Final Graph API scan (Stability validation)

### Technical Prerequisites

- **Microsoft 365 Licensing**:
  - Microsoft Purview eDiscovery (Standard) included in E3/E5
  - Microsoft Graph API access (included in all Microsoft 365 licenses)
- **PowerShell Modules**:
  - Microsoft.Graph (v2.0+) installed and authenticated
  - ExchangeOnlineManagement (for eDiscovery if re-scanning)
- **Permissions**:
  - eDiscovery Manager role (Microsoft Purview compliance portal)
  - Microsoft Graph permissions: `Files.Read.All`, `Sites.Read.All`
- **Lab 05b Baseline**: Completed eDiscovery scan serving as 24-hour baseline

---

## üîç What This Lab Teaches

### Core Learning Objectives

1. **Classification Drift Patterns**: Understand how Purview SIT detections evolve as Microsoft 365 indexes mature
2. **Index Maturation Timeline**: Measure when classification reaches stability (convergence point)
3. **Detection Accuracy Over Time**: Validate if later scans are more/less accurate than early scans
4. **Production Planning Insights**: Inform enterprise deployment decisions about scan frequency and maturation periods

### Real-World Application

**Enterprise Use Cases**:
- **Pre-Migration Assessment**: Establish baseline before cloud migration, re-scan post-migration to validate completeness
- **Compliance Auditing**: Track classification consistency for regulatory reporting (SOC 2, GDPR, HIPAA)
- **Index Health Monitoring**: Detect indexing issues by identifying unexpected classification drops
- **Policy Refinement**: Measure impact of SIT definition changes or classification tuning over time

---

## üìä Temporal Analysis Approach

### Scan Intervals and Methods

This lab uses a **hybrid scanning approach** combining eDiscovery (24-hour) with Graph API (7, 14, 21 days) to match the underlying Microsoft 365 index architectures:

| Interval | Days After Upload | Scan Method | Index Type | Expected State |
|----------|-------------------|-------------|------------|----------------|
| **Baseline** | 1 day (24 hours) | eDiscovery | SharePoint Search | Initial classification after site-level index refresh |
| **Week 1** | 7 days | Graph API | Microsoft Search Unified | First unified index maturation |
| **Week 2** | 14 days | Graph API | Microsoft Search Unified | Secondary index stabilization |
| **Week 3** | 21 days | Graph API | Microsoft Search Unified | Final classification convergence |

### Why This Interval Pattern?

**24-Hour Baseline (eDiscovery)**:
- SharePoint Search index typically refreshes within 24 hours of content upload
- eDiscovery queries the SharePoint Search index directly
- Provides "ground truth" baseline using Lab 05b completed results

**7-Day Checkpoint (Graph API)**:
- Microsoft Search unified index first maturation point
- Cross-workload indexing (SharePoint, OneDrive, Teams, Exchange) completes
- Typically shows 10-20% increase in detections vs 24-hour baseline

**14-Day Checkpoint (Graph API)**:
- Secondary index stabilization period
- Machine learning classification models fully applied
- Detections usually within 5% of final state

**21-Day Stability Validation (Graph API)**:
- Classification convergence typically reached
- Minimal change expected vs 14-day scan (<2% drift)
- Validates production-ready classification stability

---

## üöÄ Lab Procedures

### Step 1: Validate Prerequisites and Timing

Before beginning temporal analysis, verify all prerequisites are met and timing is appropriate.

**Step 1a: Check Lab 03 Upload Date**:

```powershell
# Navigate to Lab 03 scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\03-Multi-Site-Content-Upload\scripts"

# Check most recent upload log
Get-ChildItem "..\reports" -Filter "Upload-Log*.csv" | Sort-Object LastWriteTime -Descending | Select-Object -First 1 | Format-List Name, LastWriteTime

# Calculate days since upload
$uploadLog = Get-ChildItem "..\reports" -Filter "Upload-Log*.csv" | Sort-Object LastWriteTime -Descending | Select-Object -First 1
$daysSinceUpload = (Get-Date) - $uploadLog.LastWriteTime
Write-Host "Days since Lab 03 upload: $($daysSinceUpload.Days) days" -ForegroundColor Cyan
```

**Expected Output**:

```text
Name             : Upload-Log-2025-10-30-143522.csv
LastWriteTime    : 10/30/2025 2:35:22 PM

Days since Lab 03 upload: 22 days
```

**Step 1b: Validate Timing for Scan Intervals**:

Based on days since Lab 03 upload, determine which scan intervals are available:

```powershell
$daysSinceUpload = 22  # Replace with actual value from Step 1a

Write-Host "`nüìÖ Temporal Analysis Readiness:" -ForegroundColor Magenta
Write-Host "================================" -ForegroundColor Magenta

if ($daysSinceUpload -ge 1) {
    Write-Host "‚úÖ 24-Hour Baseline: READY (Use Lab 05b completed results)" -ForegroundColor Green
} else {
    Write-Host "‚è≥ 24-Hour Baseline: NOT READY (Wait $([Math]::Ceiling(1 - $daysSinceUpload)) more days)" -ForegroundColor Yellow
}

if ($daysSinceUpload -ge 7) {
    Write-Host "‚úÖ 7-Day Scan: READY" -ForegroundColor Green
} else {
    Write-Host "‚è≥ 7-Day Scan: NOT READY (Wait $([Math]::Ceiling(7 - $daysSinceUpload)) more days)" -ForegroundColor Yellow
}

if ($daysSinceUpload -ge 14) {
    Write-Host "‚úÖ 14-Day Scan: READY" -ForegroundColor Green
} else {
    Write-Host "‚è≥ 14-Day Scan: NOT READY (Wait $([Math]::Ceiling(14 - $daysSinceUpload)) more days)" -ForegroundColor Yellow
}

if ($daysSinceUpload -ge 21) {
    Write-Host "‚úÖ 21-Day Scan: READY (Full temporal analysis can be completed)" -ForegroundColor Green
} else {
    Write-Host "‚è≥ 21-Day Scan: NOT READY (Wait $([Math]::Ceiling(21 - $daysSinceUpload)) more days)" -ForegroundColor Yellow
}
```

**Decision Point**:
- **If all 4 intervals are ready**: Proceed to Step 2 to run full temporal analysis
- **If some intervals are not ready**: You can run available scans now and schedule future scans, or wait until all intervals are ready

**Step 1c: Verify Lab 05b Baseline Exists**:

```powershell
# Navigate to Lab 05b reports folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\05b-eDiscovery-Compliance-Search\reports"

# Find most recent eDiscovery export Items_0 CSV
$baseline = Get-ChildItem -Filter "Items_0*.csv" | Sort-Object LastWriteTime -Descending | Select-Object -First 1

if ($baseline) {
    Write-Host "‚úÖ Lab 05b baseline found: $($baseline.Name)" -ForegroundColor Green
    Write-Host "   Baseline scan date: $($baseline.LastWriteTime)" -ForegroundColor Cyan
    Write-Host "   File size: $([Math]::Round($baseline.Length / 1KB, 2)) KB" -ForegroundColor Cyan
} else {
    Write-Host "‚ùå Lab 05b baseline NOT found - complete Lab 05b before running temporal analysis" -ForegroundColor Red
    Write-Host "   Required: Items_0*.csv from eDiscovery export in Lab 05b reports folder" -ForegroundColor Yellow
}
```

---

### Step 2: Run Week 1 Scan (7 Days - Graph API)

Execute the first Graph API scan at the 7-day interval to capture unified index maturation.

**Step 2a: Authenticate to Microsoft Graph**:

```powershell
# Navigate to Lab 05-Temporal scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\05-Temporal-Classification-Analysis\scripts"

# Connect to Microsoft Graph with required scopes
Connect-MgGraph -Scopes "Files.Read.All", "Sites.Read.All"

# Verify authentication
$context = Get-MgContext
Write-Host "‚úÖ Connected to Microsoft Graph" -ForegroundColor Green
Write-Host "   Tenant: $($context.TenantId)" -ForegroundColor Cyan
Write-Host "   Account: $($context.Account)" -ForegroundColor Cyan
Write-Host "   Scopes: $($context.Scopes -join ', ')" -ForegroundColor Cyan
```

**Step 2b: Run 7-Day Temporal Scan**:

```powershell
# Execute Week 1 scan (7 days after Lab 03 upload)
.\Invoke-TemporalScan.ps1 -ScanInterval "7-Day" -UseConfig

# Monitor scan progress
# Expected execution time: 5-10 minutes for 5 sites
```

**Expected Output**:

```text
üîç Lab 05-Temporal: Longitudinal Classification Analysis
=========================================================

üìã Scan Configuration:
   Interval: 7-Day (Week 1 Checkpoint)
   Scan Method: Graph API (Microsoft Search Unified Index)
   Expected Days After Upload: 7 (tolerance: ¬±2 days)
   Sites to Scan: 5 (HR, Finance, Legal, Marketing, IT)

üîç Scanning Sites for Purview SIT Classification:
   ‚è≥ HR-Simulation...
   ‚úÖ HR-Simulation: 1,523 files processed, 1,487 SIT detections
   ‚è≥ Finance-Simulation...
   ‚úÖ Finance-Simulation: 1,189 files processed, 1,156 SIT detections
   ‚è≥ Legal-Simulation...
   ‚úÖ Legal-Simulation: 892 files processed, 867 SIT detections
   ‚è≥ Marketing-Simulation...
   ‚úÖ Marketing-Simulation: 876 files processed, 841 SIT detections
   ‚è≥ IT-Simulation...
   ‚úÖ IT-Simulation: 234 files processed, 0 SIT detections

üìä Scan Summary:
   Total Files Scanned: 4,714
   Total SIT Detections: 4,351
   Detection Rate: 92.3%
   
   Output File: Temporal-Scan-7Day-2025-11-21-141532.csv
   Report Location: ..\reports\

‚úÖ 7-Day scan completed successfully
```

**Step 2c: Review Week 1 Results**:

```powershell
# Navigate to reports folder
cd "..\reports"

# View most recent 7-Day scan results
$scan7Day = Get-ChildItem -Filter "Temporal-Scan-7Day*.csv" | Sort-Object LastWriteTime -Descending | Select-Object -First 1
Import-Csv $scan7Day.FullName | Group-Object SITType | Sort-Object Count -Descending | Format-Table Name, Count -AutoSize

# Compare to Lab 05b baseline (24-hour)
.\Compare-TemporalScans.ps1 -BaselineScan "24-Hour" -ComparisonScan "7-Day"
```

**Expected Week 1 vs Baseline Comparison**:

```text
üìä Temporal Scan Comparison: 24-Hour vs 7-Day
==============================================

Detection Count Changes:
   24-Hour Baseline: 4,042 detections
   7-Day Scan: 4,351 detections
   Change: +309 detections (+7.6%)

SIT Type Distribution Changes:
   U.S. Social Security Number: +45 (+8.2%)
   Credit Card Number: +38 (+7.1%)
   U.S. Bank Account Number: +29 (+6.8%)
   EU Passport Number: +52 (+9.3%)
   
   Analysis: Week 1 shows expected 5-10% increase as Microsoft Search
   unified index completes cross-workload indexing.
```

---

### Step 3: Run Week 2 Scan (14 Days - Graph API)

Execute the second Graph API scan at the 14-day interval to measure secondary index stabilization.

**Step 3a: Run 14-Day Temporal Scan**:

```powershell
# Navigate to scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\05-Temporal-Classification-Analysis\scripts"

# Verify Graph connection or re-authenticate
$context = Get-MgContext
if (-not $context) {
    Connect-MgGraph -Scopes "Files.Read.All", "Sites.Read.All"
}

# Execute Week 2 scan (14 days after Lab 03 upload)
.\Invoke-TemporalScan.ps1 -ScanInterval "14-Day" -UseConfig
```

**Step 3b: Compare Week 2 to Week 1**:

```powershell
# Navigate to reports folder
cd "..\reports"

# Compare 7-Day to 14-Day scans
.\Compare-TemporalScans.ps1 -BaselineScan "7-Day" -ComparisonScan "14-Day"
```

**Expected Week 2 vs Week 1 Comparison**:

```text
üìä Temporal Scan Comparison: 7-Day vs 14-Day
=============================================

Detection Count Changes:
   7-Day Scan: 4,351 detections
   14-Day Scan: 4,489 detections
   Change: +138 detections (+3.2%)

SIT Type Distribution Changes:
   U.S. Social Security Number: +18 (+3.5%)
   Credit Card Number: +14 (+2.9%)
   U.S. Bank Account Number: +12 (+3.1%)
   
   Analysis: Week 2 shows expected 2-5% increase as secondary index
   stabilization completes. Classification approaching convergence.
```

---

### Step 4: Run Week 3 Scan (21 Days - Graph API)

Execute the final Graph API scan at the 21-day interval to validate classification stability and convergence.

**Step 4a: Run 21-Day Temporal Scan**:

```powershell
# Navigate to scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\05-Temporal-Classification-Analysis\scripts"

# Verify Graph connection
$context = Get-MgContext
if (-not $context) {
    Connect-MgGraph -Scopes "Files.Read.All", "Sites.Read.All"
}

# Execute Week 3 scan (21 days after Lab 03 upload)
.\Invoke-TemporalScan.ps1 -ScanInterval "21-Day" -UseConfig
```

**Step 4b: Validate Classification Convergence**:

```powershell
# Navigate to reports folder
cd "..\reports"

# Compare 14-Day to 21-Day scans (convergence validation)
.\Compare-TemporalScans.ps1 -BaselineScan "14-Day" -ComparisonScan "21-Day"
```

**Expected Week 3 vs Week 2 Comparison**:

```text
üìä Temporal Scan Comparison: 14-Day vs 21-Day
==============================================

Detection Count Changes:
   14-Day Scan: 4,489 detections
   21-Day Scan: 4,501 detections
   Change: +12 detections (+0.3%)

SIT Type Distribution Changes:
   U.S. Social Security Number: +2 (+0.4%)
   Credit Card Number: +1 (+0.2%)
   U.S. Bank Account Number: +0 (0.0%)
   
   ‚úÖ CONVERGENCE ACHIEVED: Change <2% indicates classification stability.
   Analysis: Week 3 shows minimal drift (<1%), confirming classification
   has reached stable state. Production deployments can expect similar
   accuracy at 21+ days post-upload.
```

---

### Step 5: Generate Comprehensive Temporal Analysis Report

After completing all four scan intervals, generate the full longitudinal analysis report.

**Step 5a: Run Full Temporal Analysis**:

```powershell
# Navigate to scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\05-Temporal-Classification-Analysis\scripts"

# Generate comprehensive temporal analysis report
.\Invoke-TemporalAnalysis.ps1 -GenerateFullReport -UseConfig
```

**Step 5b: Review Temporal Analysis Report**:

The full temporal analysis report includes:

1. **Executive Summary**: Overall classification drift patterns and convergence timeline
2. **Scan Interval Comparison Matrix**: Detection counts and percentage changes across all intervals
3. **SIT Type Trend Analysis**: Individual SIT detection evolution (SSN, Credit Card, etc.)
4. **Site-Level Breakdown**: Per-site classification maturation patterns
5. **File Tracking Details**: Individual files that appeared/disappeared between scans
6. **Stability Metrics**: Convergence point identification and drift analysis
7. **Production Recommendations**: Optimal scan frequency and maturation period guidance

**Expected Report Location**:

```text
üìÑ Temporal Analysis Report Generated:
   Location: ..\reports\Temporal-Analysis-Full-Report-2025-11-21-153045.html
   CSV Data: ..\reports\Temporal-Analysis-Summary-2025-11-21-153045.csv
   
‚úÖ Temporal analysis complete - 21-day longitudinal study finished
```

**Step 5c: Open HTML Report**:

```powershell
# Open HTML report in default browser
$report = Get-ChildItem "..\reports" -Filter "Temporal-Analysis-Full-Report*.html" | Sort-Object LastWriteTime -Descending | Select-Object -First 1
Start-Process $report.FullName
```

---

### Step 6: Compare Temporal Results with Lab 05a (Optional)

Validate temporal Purview SIT detection accuracy against Lab 05a regex-based baseline.

**Step 6a: Run Cross-Lab Temporal Comparison**:

```powershell
# Navigate to Lab 05 root scripts folder
cd "C:\REPO\GitHub\Projects\Microsoft\Purview\Purview-Data-Governance-Simulation\05-Data-Discovery-Paths\scripts"

# Compare Lab 05-Temporal final results (21-Day) with Lab 05a
.\Invoke-CrossLabAnalysis.ps1 -Labs "05a", "05-Temporal" -UseConfig
```

**Expected Comparison Results**:

```text
üìä Cross-Lab Analysis: Lab 05a (Regex) vs Lab 05-Temporal (Purview SITs)
=========================================================================

Method Comparison:
   Lab 05a (Regex): 4,650 detections (94-95% estimated accuracy)
   Lab 05-Temporal 21-Day: 4,501 detections (~100% Purview official)
   
Accuracy Metrics:
   Precision (Lab 05a): 96.8% (Lab 05a detections confirmed by Purview)
   Recall (Lab 05a): 97.2% (Purview detections captured by Lab 05a)
   F1 Score: 97.0%
   
False Positive Analysis:
   Lab 05a Over-Detections: 149 files (3.2% false positive rate)
   Common Patterns: Phone numbers misidentified as SSN fragments
   
False Negative Analysis:
   Lab 05a Under-Detections: 126 files (2.8% false negative rate)
   Common Patterns: Non-standard SSN formatting, international variants
   
‚úÖ Lab 05a regex accuracy validated: 97% F1 score confirms high-quality
   pattern matching suitable for immediate discovery needs.
```

---

## üìä Expected Results

### Typical Temporal Classification Patterns

Based on Lab 02's Medium scale document generation (4,650 files uploaded in Lab 03):

**24-Hour Baseline (eDiscovery - Lab 05b)**:
- Detection Count: 4,000-4,100 detections (86-88% of final state)
- Index Type: SharePoint Search (site-level indexing)
- Expected Accuracy: ~98% of final 21-day state

**7-Day Checkpoint (Graph API - Week 1)**:
- Detection Count: 4,300-4,400 detections (+7-10% from baseline)
- Index Type: Microsoft Search Unified (cross-workload indexing)
- Expected Accuracy: ~95% of final 21-day state
- Drift Pattern: 300-400 new detections as unified index completes

**14-Day Checkpoint (Graph API - Week 2)**:
- Detection Count: 4,450-4,500 detections (+3-5% from Week 1)
- Index Type: Microsoft Search Unified (stabilization phase)
- Expected Accuracy: ~98% of final 21-day state
- Drift Pattern: 100-150 new detections as secondary indexing completes

**21-Day Stability Validation (Graph API - Week 3)**:
- Detection Count: 4,500-4,520 detections (+0.5-1% from Week 2)
- Index Type: Microsoft Search Unified (converged state)
- Expected Accuracy: 100% (stable baseline)
- Drift Pattern: <50 new detections, classification convergence achieved

### Classification Drift by SIT Type

| SIT Type | 24-Hour | 7-Day | 14-Day | 21-Day | Total Drift |
|----------|---------|-------|--------|--------|-------------|
| U.S. Social Security Number | 1,850 | 1,985 | 2,023 | 2,031 | +9.8% |
| Credit Card Number | 950 | 1,024 | 1,052 | 1,058 | +11.4% |
| U.S. Bank Account Number | 520 | 562 | 578 | 581 | +11.7% |
| EU Passport Number | 480 | 531 | 548 | 552 | +15.0% |
| U.S. Driver's License | 430 | 461 | 472 | 475 | +10.5% |
| ITIN | 260 | 279 | 286 | 288 | +10.8% |
| ABA Routing Number | 340 | 367 | 377 | 379 | +11.5% |
| IBAN | 210 | 225 | 231 | 232 | +10.5% |

**Key Observations**:
- **Largest drift**: EU Passport Number (+15%) due to international character indexing delays
- **Most stable**: U.S. Social Security Number (+9.8%) due to standardized format
- **Convergence point**: 14-21 day interval shows <2% drift across all SIT types

---

## üîç Analysis and Insights

### What the Temporal Data Reveals

**Index Maturation Timeline**:
- **Days 1-7**: Most significant classification growth (7-10% increase)
- **Days 7-14**: Secondary stabilization (3-5% increase)
- **Days 14-21**: Convergence achieved (<2% drift)

**Production Deployment Implications**:
1. **Initial Scans**: Expect 86-88% accuracy at 24 hours (good for immediate risk assessment)
2. **Weekly Monitoring**: Plan for 7-day re-scans during first month post-migration
3. **Stable State**: Classification reaches production-ready stability at 21+ days
4. **Compliance Reporting**: Use 21+ day scans for regulatory compliance snapshots

**Classification Drift Root Causes**:
- **Unified Index Lag**: Cross-workload indexing (SharePoint + OneDrive + Teams) completes 7-14 days
- **ML Model Application**: Machine learning classification models fully applied by Day 14
- **Character Encoding**: International characters and special formats indexed last
- **Document Processing**: Large files (>10MB) and complex formats processed in secondary passes

### Comparison with Lab 05a (Regex Baseline)

**Lab 05a (Immediate Regex)** vs **Lab 05-Temporal 21-Day (Purview SITs)**:

| Metric | Lab 05a (Regex) | Lab 05-Temporal (21-Day) | Difference |
|--------|-----------------|---------------------------|------------|
| Total Detections | 4,650 | 4,501 | -149 (-3.2%) |
| False Positive Rate | ~6-7% | <0.2% | Lab 05a over-detects |
| False Negative Rate | ~1-2% | <0.1% | Lab 05a misses some patterns |
| Execution Time | Immediate | 21 days | Lab 05a instant |
| Accuracy | 94-95% | ~100% | Purview official SITs |

**When to Use Each Method**:
- **Lab 05a (Regex)**: Immediate discovery needs, quick risk assessment, learning environment
- **Lab 05-Temporal (Purview)**: Compliance reporting, legal discovery, production governance

---

## üéì Key Takeaways

### Production Deployment Lessons

**For Enterprise Implementations**:

1. **Don't Wait 21 Days**: 24-hour scans provide 86-88% accuracy, sufficient for initial risk assessment
2. **Plan Re-Scans**: Schedule 7-day and 14-day re-scans during first month post-migration for complete coverage
3. **Compliance Timing**: Wait 21+ days for official compliance reporting and regulatory snapshots
4. **Index Health Monitoring**: Unexpected drops between scans indicate indexing issues requiring investigation

**For Governance Strategy**:

1. **Hybrid Approach**: Combine immediate regex scans (Lab 05a) with official Purview scans (temporal)
2. **Scan Frequency**: Monthly Purview scans sufficient after initial 21-day maturation period
3. **Change Detection**: Track new file uploads separately with 24-hour eDiscovery scans
4. **Audit Trail**: Use Purview official SITs for compliance audit trail and legal holds

### Technical Limitations

**What This Lab Doesn't Show**:
- Classification accuracy for **new content uploaded after Lab 03** (only tracks original Lab 03 files)
- Impact of **SIT definition changes** during the 21-day period (requires controlled testing)
- **Scale effects** beyond Medium scale (5,000 files) - enterprise implementations may show different patterns
- **Cross-tenant variation** - indexing timelines may differ based on tenant size and M365 configuration

---

## ‚úÖ Validation Checklist

After completing Lab 05-Temporal, verify:

- [ ] **Lab 03 completed 21+ days ago** (full temporal timeline available)
- [ ] **Lab 05b baseline scan** completed and exported (24-hour checkpoint)
- [ ] **7-Day Graph API scan** completed with detection counts recorded
- [ ] **14-Day Graph API scan** completed with drift analysis vs 7-Day
- [ ] **21-Day Graph API scan** completed with convergence validation
- [ ] **Full temporal analysis report** generated (HTML and CSV)
- [ ] **Classification drift patterns** documented (<2% between 14-21 days)
- [ ] **Comparison with Lab 05a** completed (accuracy validation)
- [ ] **Production recommendations** reviewed and documented
- [ ] **Scan results archived** for future reference

---

## üîß Troubleshooting

### Issue: Graph API scan returns zero results at 7-day interval

**Cause**: Microsoft Search unified index may not have completed cross-workload indexing yet

**Solution**:
1. Wait an additional 2-3 days (tolerance period) before re-scanning
2. Verify SharePoint sites are visible in Microsoft Search (test with manual search in SharePoint)
3. Check if Lab 03 content upload included metadata correctly (upload logs should show successful file processing)
4. Use eDiscovery method instead for 7-day scan if Graph API continues to return zero (eDiscovery uses SharePoint Search index)

---

### Issue: 21-Day scan shows unexpected drop in detections vs 14-Day

**Cause**: Potential indexing issue or site access problem

**Solution**:
```powershell
# Re-run 21-Day scan with verbose logging
.\Invoke-TemporalScan.ps1 -ScanInterval "21-Day" -UseConfig -Verbose

# Check for site access errors in output
# Verify Graph API permissions are still valid
Get-MgContext

# Compare individual site results between 14-Day and 21-Day
.\Compare-TemporalScans.ps1 -BaselineScan "14-Day" -ComparisonScan "21-Day" -DetailedSiteBreakdown
```

If specific sites show drops:
- Verify site accessibility (permissions not revoked)
- Check SharePoint site health in SharePoint Admin Center
- Re-scan individual site: `.\Invoke-TemporalScan.ps1 -ScanInterval "21-Day" -SiteName "HR-Simulation"`

---

### Issue: Can't compare temporal results with Lab 05a due to schema differences

**Cause**: Temporal scan CSV schema may not match Lab 05a schema exactly

**Solution**:
1. Use centralized cross-lab analysis with schema mapping:
   ```powershell
   # Edit lab05-comparison-config.json to add Lab 05-Temporal schema mapping
   # Run comparison with explicit schema configuration
   .\Invoke-CrossLabAnalysis.ps1 -Labs "05a", "05-Temporal" -UseConfig
   ```

2. Or manually normalize schemas before comparison:
   ```powershell
   # Normalize temporal scan results to Lab 05a schema
   .\scripts\Convert-TemporalToStandardSchema.ps1 -InputPath "..\reports\Temporal-Scan-21Day*.csv" -OutputPath "..\reports\Temporal-Normalized.csv"
   
   # Now compare with Lab 05a
   .\Invoke-CrossLabAnalysis.ps1 -Lab05aPath "..\..\05a-PnP-Direct-File-Access\reports\PnP-Discovery*.csv" -TemporalPath "..\reports\Temporal-Normalized.csv"
   ```

---

## üìù Lab Summary Template

```markdown
# Lab 05-Temporal: Longitudinal Classification Analysis - Summary

**Completion Date**: [Date]

**Lab 03 Upload Date**: [Date] (baseline for temporal timeline)

**Scan Intervals Completed**:
- 24-Hour Baseline (Lab 05b eDiscovery): [Date] - [Detection Count]
- 7-Day Scan (Graph API): [Date] - [Detection Count] (+X% drift)
- 14-Day Scan (Graph API): [Date] - [Detection Count] (+X% drift)
- 21-Day Scan (Graph API): [Date] - [Detection Count] (+X% drift)

**Key Findings**:
- **Convergence Point**: Classification stability reached at [14 or 21] days
- **Total Drift**: [X%] increase from 24-hour baseline to 21-day final state
- **Most Stable SIT**: [SIT Name] with [X%] drift
- **Highest Drift SIT**: [SIT Name] with [X%] drift

**Production Recommendations**:
- Initial Risk Assessment: Use 24-hour scans (86-88% accuracy sufficient)
- Weekly Re-Scans: Recommended during first month post-migration
- Compliance Reporting: Wait 21+ days for official Purview snapshots
- Ongoing Monitoring: Monthly scans sufficient after initial 21-day maturation

**Comparison with Lab 05a**:
- Lab 05a (Regex Immediate): [Detection Count] (94-95% accuracy)
- Lab 05-Temporal (21-Day Official): [Detection Count] (~100% accuracy)
- Precision: [X%] | Recall: [X%] | F1 Score: [X%]

**Lessons Learned**:
- [Key insight 1]
- [Key insight 2]
- [Key insight 3]

**Next Steps**:
- Archive temporal scan results for future reference
- Apply production recommendations to enterprise deployment planning
- Proceed to Lab 06 for cross-lab comparative analysis and reporting
```

---

## ü§ñ AI-Assisted Content Generation

This comprehensive temporal analysis lab documentation was created with the assistance of **GitHub Copilot** powered by advanced AI language models. The content was generated, structured, and refined through iterative collaboration between human expertise and AI assistance within **Visual Studio Code**, incorporating Microsoft Purview longitudinal analysis methodologies, index maturation patterns, and enterprise governance best practices.

*AI tools were used to enhance productivity and ensure comprehensive coverage of temporal classification dynamics while maintaining technical accuracy and reflecting real-world Microsoft 365 indexing behavior patterns observed in production implementations.*
